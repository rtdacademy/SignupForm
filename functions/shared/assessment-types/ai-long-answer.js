/**
 * Reusable AI Long Answer Assessment Module
 * This module provides a factory function to create AI-powered long answer assessments
 * Course developers can import this and configure it with their specific prompts and settings
 * 
 * ARCHITECTURE:
 * =============
 * This backend module works in conjunction with the frontend component:
 * - Frontend: src/FirebaseCourses/components/assessments/AILongAnswerQuestion/index.js
 * - Backend: This module creates cloud functions that the frontend calls
 * - Configuration: Set parameters in course-specific files like functions/courses/2/02-momentum-one-dimension/assessments.js
 * 
 * The frontend component automatically handles:
 * - Displaying questions with rubrics generated by this module
 * - Student answer submission with rich text support
 * - AI-powered evaluation and scoring
 * - Detailed feedback display per rubric criterion
 * - AI chat integration with answer context
 * 
 * WORKFLOW:
 * =========
 * 1. Configure this module in your course assessment file
 * 2. Deploy the cloud function
 * 3. Frontend component calls your cloud function to generate questions with rubrics
 * 4. Students write answers using the rich text editor
 * 5. AI evaluates answers against the rubric criteria
 * 6. Students receive detailed scoring and feedback
 * 
 * USAGE:
 * ======
 * 
 * ```javascript
 * const { createAILongAnswer } = require('../../../shared/assessment-types/ai-long-answer');
 * 
 * exports.yourFunctionName = createAILongAnswer({
 *   // Configuration object - see ACCEPTED PARAMETERS below
 * });
 * ```
 * 
 * ACCEPTED PARAMETERS:
 * ===================
 * 
 * Core Configuration:
 * - prompts: {Object} - Prompts for different difficulty levels
 *   - beginner: {string} - Prompt for beginner-level questions with rubrics
 *   - intermediate: {string} - Prompt for intermediate-level questions with rubrics
 *   - advanced: {string} - Prompt for advanced-level questions with rubrics
 * 
 * Rubric Settings:
 * - rubricCriteria: {number} - Number of rubric criteria (1-6, default: 3)
 * - pointsPerCriterion: {Array<number>} - Points for each criterion (default: auto-distributed)
 * - totalPoints: {number} - Total points for the question (default: 10)
 * 
 * Word Limits:
 * - wordLimits: {Object} - Word count constraints
 *   - min: {number} - Minimum words (default: 50)
 *   - max: {number} - Maximum words (default: 500)
 * 
 * Activity Settings:
 * - activityType: {string} - Type of activity ('lesson', 'assignment', 'exam', 'lab')
 * - maxAttempts: {number} - Maximum attempts allowed (default: from course config)
 * - showRubric: {boolean} - Whether to show rubric to students (default: true)
 * - showWordCount: {boolean} - Whether to show word count (default: true)
 * - showHints: {boolean} - Whether to enable hints (default: false)
 * - theme: {string} - Color theme ('blue', 'green', 'purple', 'amber')
 * - allowDifficultySelection: {boolean} - Allow students to select difficulty
 * 
 * AI Chat Settings:
 * - enableAIChat: {boolean} - Whether to show AI chat button for students
 * - aiChatContext: {string} - Additional context about the question to help AI tutors
 * 
 * AI Generation Settings:
 * - aiSettings: {Object} - AI generation parameters
 *   - temperature: {number} - AI creativity (0-1, default: 0.8)
 *   - topP: {number} - Nucleus sampling (0-1, default: 0.85)
 *   - topK: {number} - Top-K sampling (default: 40)
 * 
 * Content Settings:
 * - katexFormatting: {boolean} - Enable LaTeX math formatting (default: true)
 * - subject: {string} - Subject name for context
 * - gradeLevel: {number} - Grade level for context
 * - topic: {string} - Topic name for context
 * - learningObjectives: {Array<string>} - Learning objectives for context
 * 
 * Fallback Questions:
 * - fallbackQuestions: {Array<Object>} - Backup questions if AI generation fails
 *   Each fallback question should have:
 *   - questionText: {string}
 *   - rubric: {Array<Object>} - Array of {criterion, points, description} objects
 *   - maxPoints: {number}
 *   - wordLimit: {Object} - {min, max}
 *   - sampleAnswer: {string}
 *   - difficulty: {string}
 * 
 * EXAMPLE USAGE:
 * ==============
 * 
 * ```javascript
 * exports.course2_momentum_longAnswer = createAILongAnswer({
 *   prompts: {
 *     beginner: "Create a beginner physics long answer question about momentum...",
 *     intermediate: "Create an intermediate physics long answer question...",
 *     advanced: "Create an advanced physics long answer question..."
 *   },
 *   activityType: 'assignment',
 *   totalPoints: 10,
 *   rubricCriteria: 4,
 *   wordLimits: { min: 100, max: 300 },
 *   enableAIChat: true,
 *   aiChatContext: "This question tests conceptual understanding of momentum conservation.",
 *   katexFormatting: true,
 *   maxAttempts: 2,
 *   theme: 'blue',
 *   subject: 'Physics 30',
 *   gradeLevel: 12,
 *   topic: 'Momentum Conservation',
 *   fallbackQuestions: [
 *     {
 *       questionText: "Explain the principle of conservation of momentum...",
 *       rubric: [
 *         { criterion: "Definition", points: 3, description: "Correctly defines momentum" },
 *         { criterion: "Conservation Law", points: 4, description: "Explains conservation principle" },
 *         { criterion: "Examples", points: 3, description: "Provides relevant examples" }
 *       ],
 *       maxPoints: 10,
 *       wordLimit: { min: 100, max: 300 },
 *       sampleAnswer: "Momentum is defined as...",
 *       difficulty: 'intermediate'
 *     }
 *   ]
 * });
 * ```
 */

const { onCall } = require('firebase-functions/v2/https');
const { genkit } = require('genkit/beta');
const { googleAI } = require('@genkit-ai/googleai');
const { z } = require('zod');
const { loadConfig } = require('../utilities/config-loader');
const { extractParameters, initializeCourseIfNeeded, getServerTimestamp, getDatabaseRef } = require('../utilities/database-utils');
const { storeSubmission, createLongAnswerSubmissionRecord } = require('../utilities/submission-storage');
const { 
  AILongAnswerQuestionSchema, 
  AILongAnswerEvaluationSchema,
  LongAnswerFunctionParametersSchema 
} = require('../schemas/assessment-schemas');
const { applyPromptModules } = require('../prompt-modules');

// Environment variables
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

// Log API key status (without exposing the actual key)
if (!GEMINI_API_KEY) {
  console.log("GEMINI_API_KEY not found in environment - will be provided via Firebase secrets in function calls");
} else {
  console.log("GEMINI_API_KEY is configured (length:", GEMINI_API_KEY.length, "characters)");
}

// Function to initialize AI with API key
function initializeAI(apiKey) {
  return genkit({
    plugins: [googleAI({ apiKey })],
    model: googleAI.model('gemini-2.0-flash'),
  });
}

/**
 * Uses Genkit with structured outputs to generate a long answer question with rubric
 * @param {Object} config - Configuration object with prompts and settings
 * @param {string} topic - The question topic
 * @param {string} difficulty - The difficulty level (beginner, intermediate, advanced)
 * @param {Array} fallbackQuestions - Course-specific fallback questions
 * @returns {Promise<Object>} Generated question with rubric and sample answer
 */
async function generateAILongAnswerQuestion(config, topic, difficulty = 'intermediate', fallbackQuestions = []) {
  try {
    // Check if API key is available
    if (!GEMINI_API_KEY) {
      console.warn("No Gemini API key found. Using fallback question instead.");
      return getFallbackLongAnswerQuestion(difficulty, fallbackQuestions, config);
    }

    // Get the appropriate prompt template from config
    const prompts = config.prompts || {};
    const promptTemplate = prompts[difficulty] || prompts.intermediate || 
      `Create a long answer question about ${topic} at ${difficulty} level.`;
    
    // Apply conditional prompt modules as system instructions
    const systemInstructions = applyPromptModules(config);
    
    // Get the rubric for this difficulty level
    const difficultyRubric = config.rubrics && config.rubrics[difficulty];
    if (!difficultyRubric) {
      console.error(`No rubric found for difficulty: ${difficulty}. Using fallback.`);
      return getFallbackLongAnswerQuestion(difficulty, fallbackQuestions, config);
    }
    
    // Calculate total points from the rubric
    const totalPoints = difficultyRubric.reduce((sum, criterion) => sum + criterion.points, 0);
    const rubricCriteria = difficultyRubric.length;
    const wordLimits = config.wordLimits || { min: 50, max: 500 };
    
    // Create clean prompt content focused on the task
    const promptText = `${promptTemplate}

    YOU MUST USE THIS EXACT RUBRIC (DO NOT MODIFY):
    ${difficultyRubric.map((criterion, index) => 
      `${index + 1}. ${criterion.criterion} (${criterion.points} points): ${criterion.description}`
    ).join('\n    ')}
    
    Total Points: ${totalPoints}

    REQUIREMENTS:
    1. Create a question that aligns perfectly with the provided rubric criteria
    2. The question should naturally lead students to address each rubric point
    3. Write a sample answer that would earn full points on all rubric criteria
    4. The sample answer should be between ${wordLimits.min} and ${wordLimits.max} words
    5. Ensure the question tests the specific skills mentioned in the rubric
    6. IMPORTANT: Students will be required to write answers between ${wordLimits.min} and ${wordLimits.max} words
    
    DO NOT create your own rubric - use the one provided above exactly as given.`;
    
    console.log("Generating AI long answer question with structured output using Genkit");
    
    try {
      // Use Genkit's structured output with our Zod schema
      const generateOptions = {
        model: googleAI.model('gemini-2.0-flash'),
        prompt: promptText,
        output: { 
          schema: AILongAnswerQuestionSchema
        },
        config: {
          temperature: config.aiSettings?.temperature || 0.6, // Lower temperature for more consistent questions
          topP: config.aiSettings?.topP || 0.85,
          topK: config.aiSettings?.topK || 40
        }
      };
      
      // Add system instructions if we have prompt modules enabled
      if (systemInstructions && systemInstructions.trim().length > 0) {
        generateOptions.system = systemInstructions;
      }
      
      const { output } = await ai.generate(generateOptions);
      
      if (output == null) {
        console.error("Genkit returned null output for AI long answer question generation");
        throw new Error("Response doesn't satisfy schema.");
      }
      
      // Enforce configured word limits and rubric
      output.wordLimit = wordLimits;
      output.rubric = difficultyRubric;
      output.maxPoints = totalPoints;
      
      console.log("Successfully generated AI long answer question with rubric:", 
        output.questionText.substring(0, 50) + "...");
      
      return {
        ...output,
        generatedBy: 'ai'
      };
      
    } catch (err) {
      console.error("Error with Genkit AI generation:", err.message);
      console.log("Falling back to predefined question due to AI generation error");
      return getFallbackLongAnswerQuestion(difficulty, fallbackQuestions, config);
    }
  } catch (error) {
    console.error("Error generating AI long answer question:", error);
    return getFallbackLongAnswerQuestion(difficulty, fallbackQuestions, config);
  }
}

/**
 * Gets a fallback long answer question from course-specific fallbacks or defaults
 * @param {string} difficulty - The difficulty level
 * @param {Array} fallbackQuestions - Course-specific fallback questions
 * @param {Object} config - Configuration object with rubrics
 * @returns {Object} A fallback question
 */
function getFallbackLongAnswerQuestion(difficulty = 'intermediate', fallbackQuestions = [], config = {}) {
  // First try course-specific fallbacks
  const filteredFallbacks = fallbackQuestions.filter(q => q.difficulty === difficulty);
  const availableFallbacks = filteredFallbacks.length > 0 ? filteredFallbacks : 
    fallbackQuestions.filter(q => q.difficulty === 'intermediate');
  
  if (availableFallbacks.length > 0) {
    const randomIndex = Math.floor(Math.random() * availableFallbacks.length);
    const selectedQuestion = availableFallbacks[randomIndex];
    
    return {
      ...selectedQuestion,
      generatedBy: 'fallback'
    };
  }
  
  // If no fallbacks available, try to use rubric from config or return a generic question
  const difficultyRubric = config.rubrics && config.rubrics[difficulty];
  const fallbackRubric = difficultyRubric || [
    {
      criterion: "Understanding",
      points: 4,
      description: "Demonstrates understanding of key concepts"
    },
    {
      criterion: "Application",
      points: 4,
      description: "Applies concepts to solve problems"
    },
    {
      criterion: "Communication",
      points: 2,
      description: "Communicates ideas clearly"
    }
  ];
  
  const maxPoints = fallbackRubric.reduce((sum, criterion) => sum + criterion.points, 0);
  
  return {
    questionText: "This is a placeholder long answer question. Please provide course-specific fallback questions.",
    rubric: fallbackRubric,
    maxPoints: maxPoints,
    wordLimit: { min: 50, max: 300 },
    sampleAnswer: "This is a placeholder sample answer. Course developers should provide fallback questions with proper sample answers.",
    generatedBy: 'placeholder'
  };
}

/**
 * Uses Genkit to evaluate a student's long answer against the rubric
 * @param {Object} question - The question object with rubric
 * @param {string} studentAnswer - The student's answer
 * @param {string} sampleAnswer - The sample answer for reference
 * @param {Object} evaluationGuidance - Course-specific evaluation guidance
 * @returns {Promise<Object>} Evaluation result with scores and feedback
 */
async function evaluateAILongAnswer(question, studentAnswer, sampleAnswer, evaluationGuidance = {}) {
  try {
    if (!GEMINI_API_KEY) {
      console.warn("No Gemini API key found. Using fallback evaluation.");
      return getFallbackEvaluation(question, studentAnswer);
    }

    // Count words in student answer
    const wordCount = studentAnswer.trim().split(/\s+/).filter(word => word.length > 0).length;
    
    // Create evaluation prompt
    const evaluationPrompt = `Evaluate this Physics 30 student answer using the rubric below.

QUESTION: ${question.questionText}

STUDENT ANSWER (${wordCount} words):
${studentAnswer}

RUBRIC:
${question.rubric.map((criterion, index) => {
  let rubricText = `${index + 1}. ${criterion.criterion} (${criterion.points} points): ${criterion.description}`;
  if (criterion.levels) {
    rubricText += '\n   Scoring Levels:';
    Object.entries(criterion.levels).forEach(([score, description]) => {
      rubricText += `\n   - ${score} points: ${description}`;
    });
  }
  return rubricText;
}).join('\n\n')}

SCORING INSTRUCTIONS:
- For each criterion, assign ONLY whole number scores (0, 1, 2, or 3) based on the scoring levels provided
- Match the student's work to the most appropriate level description
- Do NOT give partial points (no 0.5, 1.5, 2.5, etc.)
- If scoring levels are provided, you MUST use them to determine the score
- Provide brief, specific feedback for each criterion (1-2 sentences)
- Focus on content accuracy AS DEFINED BY THE RUBRIC LEVELS

After scoring each criterion:
- Overall feedback: 2 sentences summarizing performance
- Strengths: List 2-3 specific things done well (if any)
- Improvements: List 2-3 specific areas to improve
- Suggestions: ONE concise sentence (max 100 chars) on the most important improvement needed

Be objective and focus on physics content.`;

    // Build system instructions with course-specific guidance
    let systemInstructions = `You are an expert ${question.subject || 'Physics 30'} teacher evaluating student work on ${question.topic || 'physics'}.
Apply the rubric criteria strictly and consistently.
Award partial credit only when the answer partially meets the criterion description.
Be precise with scoring - if a criterion asks for "clear explanation" and the explanation is unclear, do not award full points.`;
    
    // Add common mistakes to watch for
    if (evaluationGuidance.commonMistakes && evaluationGuidance.commonMistakes.length > 0) {
      systemInstructions += `\n\nCommon student mistakes to watch for:\n${evaluationGuidance.commonMistakes.map(m => `- ${m}`).join('\n')}`;
    }
    
    // Add difficulty-specific scoring notes
    if (evaluationGuidance.scoringNotes && question.difficulty && evaluationGuidance.scoringNotes[question.difficulty]) {
      systemInstructions += `\n\nScoring guidance for ${question.difficulty} level: ${evaluationGuidance.scoringNotes[question.difficulty]}`;
    }
    
    // Add scoring reminder if present
    if (evaluationGuidance.scoringReminder) {
      systemInstructions += `\n\n${evaluationGuidance.scoringReminder}`;
    }
    
    systemInstructions += `\n${applyPromptModules({ katexFormatting: true })}`;

    console.log("Evaluating student long answer with AI");

    // Use Genkit's structured output for evaluation
    const { output } = await ai.generate({
      model: googleAI.model('gemini-2.0-flash'),
      prompt: evaluationPrompt,
      system: systemInstructions,
      output: { 
        schema: AILongAnswerEvaluationSchema
      },
      config: {
        temperature: 0.1, // Very low temperature for consistent, deterministic evaluation
        topP: 0.8,
        topK: 40
      }
    });

    if (!output) {
      throw new Error("AI evaluation returned no output");
    }

    // Validate and cap individual rubric scores to their maximum points
    output.rubricScores.forEach((score, index) => {
      const maxPointsForCriterion = question.rubric[index].points;
      if (score.score > maxPointsForCriterion) {
        console.warn(`Score for "${score.criterion}" (${score.score}) exceeds max points (${maxPointsForCriterion}). Capping to maximum.`);
        score.score = maxPointsForCriterion;
      }
      // Ensure maxPoints matches the rubric
      score.maxPoints = maxPointsForCriterion;
    });
    
    // Validate total score matches sum of rubric scores
    const calculatedTotal = output.rubricScores.reduce((sum, score) => sum + score.score, 0);
    if (Math.abs(calculatedTotal - output.totalScore) > 0.1) {
      console.warn(`Total score mismatch: calculated ${calculatedTotal}, reported ${output.totalScore}`);
      output.totalScore = calculatedTotal;
    }
    
    // Ensure total doesn't exceed maxScore from question
    if (output.totalScore > question.maxPoints) {
      console.warn(`Total score (${output.totalScore}) exceeds question max points (${question.maxPoints}). Capping to maximum.`);
      output.totalScore = question.maxPoints;
    }

    // Calculate percentage
    output.percentage = Math.round((output.totalScore / output.maxScore) * 100);

    console.log(`AI evaluation complete: ${output.totalScore}/${output.maxScore} (${output.percentage}%)`);

    return output;

  } catch (error) {
    console.error("Error evaluating long answer with AI:", error);
    console.error("Error details:", error.message);
    console.error("Error stack:", error.stack);
    
    // Try to provide more specific error information
    if (error.message && error.message.includes('schema')) {
      console.error("Schema validation error - AI output doesn't match expected structure");
    }
    
    // Log the actual error type for better debugging
    if (error.response) {
      console.error("API Response error:", error.response.status, error.response.data);
    }
    if (error.code) {
      console.error("Error code:", error.code);
    }
    
    return getFallbackEvaluation(question, studentAnswer);
  }
}

/**
 * Provides a simple fallback evaluation when AI is unavailable
 * @param {Object} question - The question object
 * @param {string} studentAnswer - The student's answer
 * @returns {Object} Basic evaluation result
 */
function getFallbackEvaluation(question, studentAnswer) {
  const wordCount = studentAnswer.trim().split(/\s+/).filter(word => word.length > 0).length;
  const hasAnswer = wordCount >= (question.wordLimit?.min || 50);
  const meetsMaxLength = wordCount <= (question.wordLimit?.max || 5000);
  
  // More nuanced scoring based on word count
  let baseScore = 0.2; // Default minimum score
  if (hasAnswer && meetsMaxLength) {
    baseScore = 0.6; // Good faith effort within word limits
  } else if (hasAnswer) {
    baseScore = 0.5; // Answer provided but too long
  }
  
  // Create more informative fallback feedback
  let overallFeedback = "Your answer has been recorded. ";
  if (!hasAnswer) {
    overallFeedback = `Your answer is below the minimum word count (${wordCount}/${question.wordLimit?.min || 50} words). `;
  } else if (!meetsMaxLength) {
    overallFeedback = `Your answer exceeds the maximum word count (${wordCount}/${question.wordLimit?.max || 500} words). `;
  }
  overallFeedback += "AI evaluation is temporarily unavailable. Your instructor will review your submission.";
  
  return {
    totalScore: Math.round(question.maxPoints * baseScore),
    maxScore: question.maxPoints,
    percentage: Math.round(baseScore * 100),
    overallFeedback: overallFeedback,
    rubricScores: question.rubric.map(criterion => ({
      criterion: criterion.criterion,
      score: Math.round(criterion.points * baseScore),
      maxPoints: criterion.points,
      feedback: "Pending instructor review. AI evaluation temporarily unavailable."
    })),
    strengths: hasAnswer ? ["Answer submitted within word count requirements"] : [],
    improvements: !hasAnswer ? ["Ensure your answer meets the minimum word count"] : [],
    suggestions: "This is a provisional score. Your instructor will provide detailed feedback on your submission."
  };
}

/**
 * Infers activity type from assessment ID patterns
 * @param {string} assessmentId - The assessment identifier
 * @returns {string} The inferred activity type
 */
function inferActivityTypeFromAssessmentId(assessmentId) {
  if (!assessmentId) return 'assignment';
  
  const id = assessmentId.toLowerCase();
  
  if (id.includes('assignment') || id.includes('homework') || id.includes('hw')) {
    return 'assignment';
  } else if (id.includes('exam') || id.includes('test') || id.includes('final')) {
    return 'exam';
  } else if (id.includes('lab') || id.includes('laboratory') || id.includes('experiment')) {
    return 'lab';
  } else if (id.includes('lesson') || id.includes('practice')) {
    return 'lesson';
  } else {
    return 'assignment'; // Default for long answer
  }
}

/**
 * Core business logic for AI Long Answer assessments
 * This can be called directly by other systems without Firebase wrapper
 */
class AILongAnswerCore {
  constructor(config = {}) {
    this.config = config;
  }

  async handleGenerate(params) {
    // Load and merge configurations
    const globalConfig = await loadConfig();
    
    // SECURITY: Use hardcoded activity type from course config
    const activityType = this.config.activityType || inferActivityTypeFromAssessmentId(params.assessmentId) || 'assignment';
    
    console.log(`Using activity type: ${activityType} (Source: ${this.config.activityType ? 'hardcoded' : 'inferred'})`);
    
    // Get activity-specific configuration
    const activityConfig = this.config.activityTypes?.[activityType] || this.config.activityTypes?.assignment || {};
    
    // Get long answer specific settings from activity config
    const longAnswerDefaults = activityConfig.longAnswer || {};
    
    const config = {
      ...globalConfig.questionTypes?.longAnswer?.ai_generated || {},
      ...activityConfig,
      ...longAnswerDefaults, // Apply long answer defaults from course config
      ...this.config // Allow courseConfig to override if needed
    };

    // Initialize course if needed
    await initializeCourseIfNeeded(params.studentKey, params.courseId, params.isStaff);

    // Reference to the assessment in the database
    const assessmentRef = getDatabaseRef('studentAssessment', params.studentKey, params.courseId, params.assessmentId, params.isStaff);
    console.log(`Database path: ${params.isStaff ? 'staff_testing' : 'students'}/${params.studentKey}/courses/${params.courseId}/Assessments/${params.assessmentId}`);

    // Check if this is a regeneration or a new assessment
    const existingAssessmentSnapshot = await assessmentRef.once('value');
    const existingAssessment = existingAssessmentSnapshot.val();
    const isRegeneration = !!existingAssessment;
    
    // Initialize the attempts counter
    let currentAttempts = 0;
    if (existingAssessment) {
      currentAttempts = existingAssessment.attempts || 0;
    }
    
    console.log(`This is a ${isRegeneration ? 'regeneration' : 'new question'} request. Current attempts: ${currentAttempts}`);
    
    // Determine max attempts - use the same value as course config for multiple choice
    let maxAttempts = config.maxAttempts || activityConfig.maxAttempts || 3; // Default 3 for long answer
    
    console.log(`Max attempts allowed: ${maxAttempts}`);
    
    // Verify the student hasn't exceeded the max attempts
    if (isRegeneration && currentAttempts >= maxAttempts) {
      console.log(`Security check: Student has exceeded max attempts (${currentAttempts}/${maxAttempts})`);
      throw new Error(`Maximum attempts (${maxAttempts}) reached for this assessment.`);
    }
    
    // Generate the AI long answer question
    console.log(`Generating AI long answer question on topic: ${params.topic}, difficulty: ${params.difficulty}`);
    const question = await generateAILongAnswerQuestion(
      config,
      params.topic,
      params.difficulty,
      this.config.fallbackQuestions || []
    );
    
    // Create the final question data object to save
    const questionData = {
      timestamp: getServerTimestamp(),
      questionText: question.questionText,
      rubric: question.rubric,
      maxPoints: question.maxPoints,
      wordLimit: question.wordLimit,
      topic: params.topic,
      subject: config.subject || 'Physics 30',
      difficulty: params.difficulty,
      generatedBy: question.generatedBy || 'ai',
      attempts: currentAttempts,
      status: 'active',
      maxAttempts: maxAttempts,
      activityType: activityType,
      enableAIChat: config.enableAIChat,
      aiChatContext: config.aiChatContext,
      settings: {
        showRubric: config.showRubric !== undefined ? config.showRubric : (longAnswerDefaults.showRubric !== undefined ? longAnswerDefaults.showRubric : true),
        showWordCount: config.showWordCount !== undefined ? config.showWordCount : (longAnswerDefaults.showWordCount !== undefined ? longAnswerDefaults.showWordCount : true),
        showHints: config.showHints === true || activityConfig.enableHints === true,
        allowDifficultySelection: config.allowDifficultySelection || activityConfig.allowDifficultySelection || false,
        theme: config.theme || activityConfig.theme || 'purple',
      }
    };
    
    // Store public question data in the database (student-accessible)
    await assessmentRef.set(questionData);

    // Store the secure data separately (server-side only)
    const secureRef = getDatabaseRef('secureAssessment', params.courseId, params.assessmentId, params.studentKey);
    
    await secureRef.set({
      sampleAnswer: question.sampleAnswer,
      hints: question.hints || [],
      timestamp: getServerTimestamp()
    });

    return {
      success: true,
      questionGenerated: true,
      assessmentId: params.assessmentId,
      generatedBy: question.generatedBy
    };
  }

  async handleEvaluate(params) {
    // Initialize course if needed
    await initializeCourseIfNeeded(params.studentKey, params.courseId, params.isStaff);

    // Reference to the assessment in the database
    const assessmentRef = getDatabaseRef('studentAssessment', params.studentKey, params.courseId, params.assessmentId, params.isStaff);
    
    // Get the existing question data
    const assessmentSnapshot = await assessmentRef.once('value');
    const assessmentData = assessmentSnapshot.val();

    if (!assessmentData) {
      throw new Error('Assessment not found');
    }
    
    // Check max attempts - but allow the current attempt to be evaluated
    const maxAttempts = assessmentData.maxAttempts || 3;
    const currentAttempts = assessmentData.attempts || 0;
    
    // Only reject if they've ALREADY submitted the maximum number of times
    // This allows their final attempt to be evaluated
    if (currentAttempts > maxAttempts) {
      console.log(`Security check: Student has exceeded max attempts (${currentAttempts}/${maxAttempts})`);
      return {
        success: false,
        error: 'Maximum attempts exceeded',
        attemptsRemaining: 0
      };
    }
    
    console.log(`Processing attempt ${currentAttempts + 1} of ${maxAttempts}`);

    // Get the secure data
    const secureRef = getDatabaseRef('secureAssessment', params.courseId, params.assessmentId, params.studentKey);
    const secureSnapshot = await secureRef.once('value');
    const secureData = secureSnapshot.val();

    if (!secureData || !secureData.sampleAnswer) {
      throw new Error('Secure assessment data not found');
    }

    // Validate answer length
    const wordCount = params.answer.trim().split(/\s+/).filter(word => word.length > 0).length;
    const wordLimit = assessmentData.wordLimit || { min: 50, max: 500 };
    
    if (wordCount < (wordLimit.min || 0)) {
      throw new Error(`Answer too short. Minimum ${wordLimit.min} words required, you wrote ${wordCount} words.`);
    }
    
    if (wordCount > (wordLimit.max || 5000)) {
      throw new Error(`Answer too long. Maximum ${wordLimit.max} words allowed, you wrote ${wordCount} words.`);
    }

    // Evaluate the answer using AI with course-specific guidance
    const evaluation = await evaluateAILongAnswer(
      assessmentData,
      params.answer,
      secureData.sampleAnswer,
      this.config.evaluationGuidance || {}
    );

    // Increment attempts
    let updatedAttempts = currentAttempts + 1;
    console.log(`Incrementing attempts from ${currentAttempts} to ${updatedAttempts}`);
    
    const attemptsRemaining = maxAttempts - updatedAttempts;

    // Create comprehensive submission record for Cloud Storage
    const submissionRecord = createLongAnswerSubmissionRecord(
      params,
      assessmentData,
      evaluation,
      updatedAttempts,
      wordCount
    );

    // Store detailed submission in Cloud Storage
    let submissionPath = null;
    try {
      submissionPath = await storeSubmission(submissionRecord);
    } catch (storageError) {
      console.warn(`âš ï¸ Failed to store submission in Cloud Storage: ${storageError.message}`);
      // Continue with assessment - storage failure shouldn't block student progress
    }

    // Update assessment data in the database (minimal data, just tracking)
    const updates = {
      attempts: updatedAttempts,
      status: evaluation.percentage >= 70 ? 'completed' : (attemptsRemaining > 0 ? 'attempted' : 'failed'),
      lastSubmission: {
        timestamp: getServerTimestamp(),
        answer: params.answer.substring(0, 100) + (params.answer.length > 100 ? '...' : ''), // Truncated for database
        wordCount: wordCount,
        totalScore: evaluation.totalScore,
        maxScore: evaluation.maxScore,
        percentage: evaluation.percentage,
        submissionPath: submissionPath // Reference to Cloud Storage file
      }
    };

    // Update the assessment
    await assessmentRef.update(updates);

    // Update the grade
    const gradeRef = getDatabaseRef('studentGrade', params.studentKey, params.courseId, params.assessmentId, params.isStaff);
    
    // For long answer, we might want to store the percentage or scaled score
    const gradeValue = evaluation.totalScore; // Or use evaluation.percentage / 10 for a 0-10 scale
    
    await gradeRef.set(gradeValue);

    // Clean up secure data if assessment is completed or all attempts exhausted
    const isCompleted = evaluation.percentage >= 70; // Long answer considers 70%+ as completed
    if (isCompleted || attemptsRemaining <= 0) {
      try {
        await secureRef.remove();
        console.log(`ðŸ—‘ï¸ Cleaned up secure assessment data for ${isCompleted ? 'completed' : 'failed'} long answer assessment: ${params.assessmentId}`);
      } catch (cleanupError) {
        console.warn(`âš ï¸ Failed to cleanup secure data for ${params.assessmentId}:`, cleanupError.message);
        // Don't throw error - cleanup failure shouldn't affect the assessment result
      }
    }

    return {
      success: true,
      result: evaluation,
      attemptsRemaining: attemptsRemaining,
      attemptsMade: updatedAttempts
    };
  }
}

/**
 * Factory function to create an AI Long Answer assessment handler
 * @param {Object} courseConfig - Course-specific configuration
 * @returns {Function} Cloud function handler
 */
function createAILongAnswer(courseConfig = {}) {
  return onCall({
    region: courseConfig.region || 'us-central1',
    timeoutSeconds: courseConfig.timeout || 120, // Longer timeout for evaluation
    memory: courseConfig.memory || '1GiB', // More memory for text processing
    enforceAppCheck: false,
  }, async (request) => {
    const data = request.data;
    const context = request;
    // Extract and validate parameters
    const params = extractParameters(data, context);
    
    // Create core handler instance
    const coreHandler = new AILongAnswerCore(courseConfig);

    // Handle question generation operation
    if (params.operation === 'generate') {
      try {
        return await coreHandler.handleGenerate(params);
      } catch (error) {
        console.error("Error generating AI long answer question:", error);
        throw new Error('Error generating question: ' + error.message);
      }
    }
    // Handle answer evaluation operation
    else if (params.operation === 'evaluate') {
      try {
        return await coreHandler.handleEvaluate(params);
      } catch (error) {
        console.error("Error evaluating answer:", error);
        throw new Error('Error evaluating answer: ' + error.message);
      }
    }

    // Invalid operation
    throw new Error('Invalid operation. Supported operations are "generate" and "evaluate".');
  });
}

module.exports = {
  createAILongAnswer,
  AILongAnswerCore
};